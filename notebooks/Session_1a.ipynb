{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction to `pandas` and `matplotlib` with example data from USGS's `dataretrieval`"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`pandas` facilitates a lot of data analysis including the powerful [`groupby()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html), which helps you use the [\"split-apply-combine\"](https://pandas.pydata.org/docs/user_guide/groupby.html) method of data analysis\n",
        "\n",
        "`matplotlib` is a popular and easy to use plotting library for Ptyhon. It resembles MATLAB plotting tools. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LERV7yiRJBQF"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np # numpy is where all the math happens"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wCbntXG4I0nW"
      },
      "source": [
        "The US Geologic Survey is making it easier to find and retrieve their data. Most development so far has been in R, but a few blessed individuals have been developing a similar package in Python, [dataretrieval](https://github.com/USGS-python/dataretrieval). Read more about [automated data retrieval](https://waterservices.usgs.gov/rest/Site-Service.html). Since I mocked up this activity for EARS33 in March 2022, there are a lot of new features and [demos](https://github.com/DOI-USGS/dataretrieval-python/tree/master/demos/hydroshare) on the repo. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataretrieval import nwis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeHKW9-IKlw6"
      },
      "source": [
        "# Part 1: EARS33 example activity"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For this activity, I demonstrate the use of nested watersheds and their gages. I grabbed three gages from nested watersheds by loookin at [this map](https://dashboard.waterdata.usgs.gov/app/nwd/en/?region=lower48&aoi=default). \n",
        "\n",
        "FYI I haven't found an easy web interface where you can see the watersheds of individual gages but the shapefiles I think are [here](https://water.usgs.gov/GIS/metadata/usgswrd/XML/gagesII_Sept2011.xml)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bu2NDLicJF5U"
      },
      "outputs": [],
      "source": [
        "# Make a list of strings corresponding to site IDs. \n",
        "# These use quotes to denote they are strings and not numbers\n",
        "# They are separated by commas and bound by square brackets\n",
        "\n",
        "sitelist = [\"01075000\", \"01081500\", \"01092000\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EV0t0soJJg3",
        "outputId": "8114e54a-3a5a-4b88-d135-71cb50d78f32"
      },
      "outputs": [],
      "source": [
        "site_info = nwis.get_record(sites=sitelist, service='site')\n",
        "\n",
        "print('Available data are:\\n', site_info.columns.values) # \"\\n\" just puts a line break in a text (string)\n",
        "print('Station names are:\\n', site_info['station_nm'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpKvkBwaJMKu"
      },
      "source": [
        "Now to Pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47L2l5H0JNeN"
      },
      "outputs": [],
      "source": [
        "# Initialize a blank dataframe object\n",
        "df = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1HwtC5qJTdc"
      },
      "source": [
        "Parameter codes for the USGS are here:https://help.waterdata.usgs.gov/codes-and-parameters/parameters To get discharge, use \"00060\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-KD-vVbJPjf"
      },
      "outputs": [],
      "source": [
        "# This loop will iterate through each of the objects in sitelist and assign it \n",
        "# a number \"i\" as it assigns the variable siteNumber to the sitelist object\n",
        "\n",
        "for i, siteNumber in enumerate(sitelist):\n",
        "    # This is the parameter code for discharge in cfs\n",
        "    parameterCode = \"00060\"\n",
        "    # These are strings of dates\n",
        "    startDate = \"2022-01-01\"\n",
        "    endDate = \"2022-12-31\"\n",
        "    # Make a temporary dataframe to store the records of each site\n",
        "    df_temp = nwis.get_record(sites=siteNumber, service='dv', start=startDate, end=endDate, parameterCd='00060')\n",
        "    # And then append each site's data to the previous sites' data\n",
        "    df = df.append(df_temp)\n",
        "    # This method keeps our dataframe compact - each of the sites have the same\n",
        "    # data, and we can always parse by site number later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "6qHxxh0yJYRV",
        "outputId": "91d8512d-3acd-4d9d-c417-09fd92b70e2f"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iI17_JdKFS8"
      },
      "source": [
        "Provisional data might be bad numbers for discharge:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9Xx6HPmKIev",
        "outputId": "8053a5ed-ba72-42c9-a74b-5ae32272dc26"
      },
      "outputs": [],
      "source": [
        "df.groupby(by='site_no')['00060_Mean'].min()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QauwJcwKQrj"
      },
      "source": [
        "-999999.0 isn't real, so let's replace with NaN (not zero!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVD-gzmOJ_de"
      },
      "outputs": [],
      "source": [
        "df.replace(-999999.0, np.nan, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O70DUc67KXJA"
      },
      "outputs": [],
      "source": [
        "df.groupby(by='site_no')['00060_Mean'].min()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F52oOE51Jcb3"
      },
      "source": [
        "Now we're going to do a groupby that will [group rows with similar values](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html) for a certain column for analysis. After we groupby, we can query the values of a different column, but grouped by the first column.\n",
        "\n",
        "In this case the code is going to make three separate plots, one for each site_no, of the column 00060_Mean, or mean daily discharge.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "Q2sT1CweJb1i",
        "outputId": "b2685b41-45b1-42b2-8da0-701bcb3c63c2"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(15,4))\n",
        "\n",
        "for site, group in df.groupby(by='site_no'):\n",
        "    \n",
        "    # site_info = nwis.get_record(sites=site, service='site')\n",
        "    # To do - get name and drainage area as label instead of station label!\n",
        "\n",
        "    group.plot(y='00060_Mean', ax=ax, legend=True, label=site)\n",
        "\n",
        "ax.set_xlabel('Date')\n",
        "ax.set_ylabel('Streamflow (cfs)')\n",
        "\n",
        "#https://stackoverflow.com/questions/39902522/pandas-groupby-object-in-legend-on-plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qUey8UsJyX9"
      },
      "source": [
        "Very cool!\n",
        "\n",
        "1. Which plot is the lowest drainage area? The highest? How do you know?\n",
        "2. How might you write an algorithm to determine the timing between peak discharges between upstream and downstream sites?\n",
        "3. Are the upstream sites always a consistent percentage of the downstream sites' discharges? Why or why not? How might that be explained? What other data might you need to test this idea? BONUS: How might you write an algorithm to track how the relative streamflows change with time? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGHbxqK4KpqS"
      },
      "source": [
        "# Part 2: discharge trends in Alaskan rivers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-V69Ug0iNZJF"
      },
      "source": [
        "So I used this search page to create a table of all Alaska gage sites and then use Pandas to parse the resulting table: https://waterdata.usgs.gov/ak/nwis/current?submitted_form=introduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fehuS2XcMdYv"
      },
      "outputs": [],
      "source": [
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mP7CPEcrMERz"
      },
      "outputs": [],
      "source": [
        "url = 'https://waterdata.usgs.gov/ak/nwis/current?index_pmcode_STATION_NM=1&index_pmcode_DATETIME=2&group_key=basin_cd&format=sitefile_output&sitefile_output_format=html_table&column_name=agency_cd&column_name=site_no&column_name=station_nm&sort_key_2=site_no&html_table_group_key=NONE&rdb_compression=file&list_of_search_criteria=realtime_parameter_selection'\n",
        "\n",
        "usgs_url = requests.get(url)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[`pd.read_html()`](https://pandas.pydata.org/docs/reference/api/pandas.read_html.html) parses for tables, which is why I generated a table (and not the tab-delimeted text, which seems like that should have worked but whatever)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vp_W39diMeyg"
      },
      "outputs": [],
      "source": [
        "# This returns a LIST of dataframes\n",
        "usgs_data = pd.read_html(usgs_url.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "qwcYhSElM62I",
        "outputId": "67ee1855-f8bf-48c9-ebdc-cf8260eaf904"
      },
      "outputs": [],
      "source": [
        "# So let's look at the first (and only) dataframe that comes out\n",
        "usgs_data = usgs_data[0]\n",
        "\n",
        "usgs_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RI8B4cK6MLMA",
        "outputId": "bc3198b0-840c-449e-c9b9-7f63a178f886"
      },
      "outputs": [],
      "source": [
        "# get_record takes a string\n",
        "site_string = str(usgs_data.iloc[0][\"Site Number\"])\n",
        "# And iloc[0] is \"the data at index location 0\"\n",
        "site_info = nwis.get_record(sites=site_string, service='site')\n",
        "print(\"Lat: \", site_info['dec_lat_va'])\n",
        "print(\"Long: \", site_info['dec_long_va'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBOcq4WzO3ue"
      },
      "outputs": [],
      "source": [
        "# Get the discharge history for the site and clean bad data\n",
        "df_hist = nwis.get_record(sites=site_string, service='dv', start='1900-01-01', end=endDate, parameterCd='00060')\n",
        "df_hist.replace(-999999.0, np.nan, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "iu7pBgsgRM_C",
        "outputId": "638a5ed7-00e1-48e3-975b-c3ccd719d08c"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(15,4))\n",
        "data = df_hist.plot(y='00060_Mean', ax=ax)\n",
        "ax.set_xlabel('Date')\n",
        "ax.set_ylabel('Streamflow (cfs)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "XA3OBdj6Qino",
        "outputId": "05e2e96f-7fe6-468c-b84f-87ddcd8b9121"
      },
      "outputs": [],
      "source": [
        "#pandas datetimeindex docs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DatetimeIndex.html\n",
        "df_hist['year'] = pd.DatetimeIndex(pd.to_datetime(df_hist.index)).year\n",
        "\n",
        "# This is a slightly weird thing, where the datetime is stored as an \"index\"\n",
        "# of the dataframe rather than a column, so I first turned the index into \n",
        "# a datetime object with the pd.to_datetime(df_hist.index) call, \n",
        "# and then parsed that datetime object for the year\n",
        "\n",
        "df_hist.head() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "id": "bXLWUCfcRZN7",
        "outputId": "7da5eefd-057b-41c3-e559-d1783ff73fea"
      },
      "outputs": [],
      "source": [
        "# Here I'm going to ask \"What is the maximum value for each year?\" the groupby()\n",
        "# function puts all the things that have the same value in a specified column\n",
        "# and then finds the max value, in this case \n",
        "# Read more here: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html\n",
        "\n",
        "annual_max_floods = df_hist.groupby('year').max().reset_index()\n",
        "\n",
        "# The groupby() function creates this new mini-dataframe, \"annual_max_floods\",\n",
        "# which can be manipulated just like our original dataframe, df_hist\n",
        "\n",
        "\n",
        "# the reset_index() call is just a little nuance where I don't want it to turn\n",
        "# my year groups into the index of the dataframe, or else Pandas will thing\n",
        "# I want to plot variables as a timeline, which I don't want to do \n",
        "\n",
        "annual_max_floods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "id": "uSdo5b-CRerm",
        "outputId": "0a4b9def-f4bf-4de4-9100-a3bbea866dbe"
      },
      "outputs": [],
      "source": [
        "# Here I'm asking Pandas to rank the discharge column in descending order...\n",
        "\n",
        "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rank.html\n",
        "annual_max_floods['rank'] = annual_max_floods['00060_Mean'].rank(ascending=False)\n",
        "\n",
        "# ...and then show them to me sorted!\n",
        "\n",
        "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sort_values.html\n",
        "annual_max_floods.sort_values(by=['rank'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "q5aPr1NsRoJ3",
        "outputId": "37cc3022-2be2-4e33-a806-3f1a0b82d5d7"
      },
      "outputs": [],
      "source": [
        "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.scatter.html\n",
        "annual_max_floods.plot.scatter(x='rank',y='00060_Mean')\n",
        "\n",
        "# Why don't you edit the above code to:\n",
        "# Properly label x and y axes\n",
        "# maybe...add the year as a color for the data points?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYy83HoYUf27"
      },
      "source": [
        "Maybe this is the time to write a function that does all that work for you \"under the hood\" rather than in big code blocks because you'll want to do the same thing to the data over and over again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rru3RShRUfcL"
      },
      "outputs": [],
      "source": [
        "def get_annual_floods(site_string):\n",
        "  # site_string is the string of the gage\n",
        "  site_info = nwis.get_record(sites=site_string, service='site')\n",
        "  \n",
        "  df_hist = nwis.get_record(sites=site_string, service='dv', start='1900-01-01', end=endDate, parameterCd='00060')\n",
        "  df_hist.replace(-999999.0, np.nan, inplace=True)\n",
        "\n",
        "  df_hist['year'] = pd.DatetimeIndex(pd.to_datetime(df_hist.index)).year\n",
        "\n",
        "  annual_max_floods = df_hist.groupby('year').max().reset_index()\n",
        "\n",
        "  annual_max_floods['rank'] = annual_max_floods['00060_Mean'].rank(ascending=False)\n",
        "\n",
        "  # And then this is the output to your function: a sorted dataframe\n",
        "  return annual_max_floods.sort_values(by=['rank'])\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Can you add some lines to the function that perhaps print the name of the site when you call `get_annual_floods()`?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "2jHu0oPNV_ZE",
        "outputId": "39f30ff0-4be0-4368-c0b6-034bbd9e0bc9"
      },
      "outputs": [],
      "source": [
        "max_flood_data = get_annual_floods(str(usgs_data.iloc[0][\"Site Number\"]))\n",
        "max_flood_data.plot.scatter(x='rank',y='00060_Mean', c='year', cmap='viridis')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "fBwUdIvSVOAw",
        "outputId": "59577024-806a-4f9f-c42c-55f9249230b3"
      },
      "outputs": [],
      "source": [
        "max_flood_data = get_annual_floods(str(usgs_data.iloc[1][\"Site Number\"]))\n",
        "max_flood_data.plot.scatter(x='rank',y='00060_Mean', c='year', cmap='viridis')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        },
        "id": "MTn2kqhOV05U",
        "outputId": "c738163c-fcd4-4d80-be35-8f5ac80f2a5d"
      },
      "outputs": [],
      "source": [
        "max_flood_data = get_annual_floods(str(usgs_data.iloc[2][\"Site Number\"]))\n",
        "max_flood_data.plot.scatter(x='rank',y='00060_Mean', c='year', cmap='viridis')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgWkfgqOV3mH"
      },
      "source": [
        "Fun!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK0bUB8kSCv6"
      },
      "source": [
        "# Part 3: Hackathon prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z53o0hLuR2pT"
      },
      "source": [
        "One of the things I wonder in my work is based on climate change, will the biggest discharge events (which have historically occurred early in the growing season and are associated with snowmelt) instead be associated with rainstorms later in the growing season, when the ground is more thawed?\n",
        "\n",
        "Hackathon prompt: can you test the hypothesis that ***maximum floods are occurring later in the year?*** And **are those floods getting bigger**?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "k3sEqCdZSOJX"
      },
      "source": [
        "Ingredients:\n",
        "\n",
        "\n",
        "1.   Sitewise parsing of maximum flood dates\n",
        "2.   Some means of quantifying the trend in date of the peak flood (perhaps a [regression line fit](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.linregress.html) to the day of year of max flood?\n",
        "3.   Some means of reducing that quantification of trend down to a single number or variable that can be added to the `usgs_data` dataframe \n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "neukom",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "c4496227e9e35a1cad8f46d1878e766ce3696b74827c1cccb91d7e0ed1733d2b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
