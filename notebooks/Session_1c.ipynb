{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bringing it all together"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the last portion of this workshop session we will look at a very real example of how my students and I bring together all sorts of exploratory data analysis and tons of different kinds of Python packages - from mathematical packages like `numpy` and `scipy` to data analysis and visualization tools like `pandas` and `seaborn` to geospaital tools in `geopandas`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "import geopandas as gpd\n",
    "import seaborn as sns\n",
    "\n",
    "import scipy as sp\n",
    "from scipy import stats"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sade took this [fairly messy data](https://www2.gwu.edu/~calm/data/north.htm) and turned it into something great!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `CALM_export.csv` is actually an export from the shapefile listed on the website above - it was the least messy format of the data. But I just exported it as a .csv so we had an example of turning columns with location data into `GeoDataframes` :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= pd.read_csv('../arctic-data/CALM_export.csv')\n",
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Major cleaning!!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing Sade did was rename the columns that should be years for the ALT measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_names = np.arange(1990, 2016, 1)\n",
    "\n",
    "old_columns = data.columns[6:32]\n",
    "\n",
    "mapping = {old_columns[i]: year_names[i] for i in range(len(old_columns))}\n",
    "\n",
    "data = data.rename(columns=mapping)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, when imported or exported the \"no measurements\" turned into zeros, which is bad, and some of the data had symbols associated with them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.fillna(np.nan)\n",
    "data = data.replace(r'^\\s*$', np.nan, regex=True)\n",
    "data.replace(0.0, np.nan, inplace= True)\n",
    "data.replace(\">263\", np.nan, inplace= True)\n",
    "data.replace(\">260\", np.nan, inplace= True)\n",
    "data.replace(\">235\", np.nan, inplace= True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we want to make sure our year columns are being read as floats and not objects (strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[:, 6:32].dtypes\n",
    "\n",
    "# iloc is index location\n",
    "# The : in the first half of the bracketed list means \"all rows\"\n",
    "# and then \"column numbers 6 through 32\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops! Let's fix that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[:, 6:32] = data.iloc[:, 6:32].apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How is the active layer changing every year?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to have to get a little clever here, because each site has its own unique dataset issues - some sites are missing most years, some sites have data gaps... "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write an algorithm that grabs an x and y array for years and measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sites in range(len(data)): # for all the rows in our dataset\n",
    "\n",
    "  y_floats = np.array(data.iloc[sites, 6:32].values, dtype=float) # read in the data from columns that are yearly ALT measurements as floats\n",
    "\n",
    "  y = y_floats[~np.isnan(y_floats)] # find the indices for which that year's measurement is NOT a nan\n",
    "\n",
    "  x = year_names[~np.isnan(y_floats)] # find the corresponding years for valid data \n",
    "\n",
    "  if np.sum([np.isnan(y)==False])>10: # if there are at least 10 valid measurements for the time period\n",
    "\n",
    "    data.loc[(sites, \"average\")] = np.mean(y) # grab the mean of those measurements\n",
    "\n",
    "    res = stats.linregress(x,y) # and regress the year array against the measurement array \n",
    "    # res is the result of the linregress function and spits out a list of important numbers\n",
    "\n",
    "    data.loc[(sites, \"slope\")] = res[0] # ...and store all that data in our data frame\n",
    "    data.loc[(sites, \"intercept\")] = res[1]\n",
    "    data.loc[(sites, \"rvalue\")] = res[2]\n",
    "    data.loc[(sites, \"pvalue\")] = res[3]\n",
    "    data.loc[(sites, \"stderr\")] = res[4]\n",
    "  else: # if we don't have enough valid data, go to the next row\n",
    "    continue"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now make it a map!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any dataframe with lats and longs can be convered into a dataframe if we specify the geometry as the appropriate columns and the appropriate crs (lat long will usually be WGS84, EPSG:4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.GeoDataFrame(\n",
    "    data, geometry=gpd.points_from_xy(data.Longitude, data.Latitude), crs='epsg:4326')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what this looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GeoPandas has a simple map of the Earth built in\n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5),dpi=300)\n",
    "im = world.plot(\n",
    "    color='white', edgecolor='black', ax=ax)\n",
    "\n",
    "gdf.plot(\n",
    "        ax=ax,\n",
    "        column='slope',\n",
    "        vmin=-2,\n",
    "        vmax=2,\n",
    "        cmap='seismic',\n",
    "        s=10, #size of point\n",
    "        legend=True,\n",
    "        legend_kwds={\n",
    "            'label': \"Change in Active Layer Thickness (cm/yr) from 1991 to 2015\",\n",
    "            'orientation': \"horizontal\"\n",
    "            }\n",
    "        )\n",
    "\n",
    "ax.set_ylim(40,90)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will leave [axis labeling](https://matplotlib.org/stable/api/axes_api.html) to you :)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's export this as a shapefile for use later down the road"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A goofy little thing where the numbers have to be strings to export lol\n",
    "gdf.columns = gdf.columns.astype(str)\n",
    "\n",
    "gdf.dropna(inplace=True, subset='Site_Name')\n",
    "\n",
    "gdf.to_file(\"CALM_points.shp\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial join with our permafrost layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform a [spatial join](https://geopandas.org/en/stable/gallery/spatial_joins.html) to determine within which permafrost category each of these data points fall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permaice = gpd.read_file(\"../arctic-data/permaice.shp\")\n",
    "\n",
    "joined_data = gdf.to_crs(permaice.crs).sjoin(permaice, how=\"inner\") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize and analyze results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I asked Sade, \"Ok, so is continuous permafrost thawing faster or slower than discontinuous permafrost?\" Let's see what she came up with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make individual data frames for each extent category - it's just a little easier this way\n",
    "df_cont=joined_data.loc[joined_data['EXTENT']=='C']\n",
    "df_discont=joined_data.loc[joined_data['EXTENT']=='D']\n",
    "df_iso=joined_data.loc[joined_data['EXTENT']=='I']\n",
    "df_spor=joined_data.loc[joined_data['EXTENT']=='S']\n",
    "\n",
    "# And make them a list\n",
    "df_extent_list=[df_cont,df_discont,df_iso,df_spor]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, she wrote a function to plot each type of permafrost in one of the four plot boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_site_timeseries(df, axis):\n",
    "  for sites in range(len(df)):\n",
    "    y_floats = np.array(df.iloc[sites, 6:32].values, dtype=float)\n",
    "    y = y_floats[~np.isnan(y_floats)]\n",
    "    x = year_names[~np.isnan(y_floats)]\n",
    "    if np.sum([np.isnan(y)==False])>10:\n",
    "      fig=sns.regplot(x=x,y=y, ax=axis, \n",
    "                      color= color_dict[df.iloc[sites]['EXTENT']], scatter_kws={'alpha':0.5, \"s\":.25}, line_kws={\"lw\":.25})\n",
    "    else:\n",
    "      continue\n",
    "      # fig.set(xlabel='Year', ylabel='Active Layer Thickness', title ='Change in ALT from 1991-2015', ylim=(0, 200)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then ran that function on each category for a figure!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a color dictionary that tells the function which color to make which category\n",
    "color_dict = {'C':'r', 'D':'g', 'I':'b' , 'S':'k', 'no_data':'grey'}\n",
    "\n",
    "fig, ax = plt.subplots(2,2, dpi=150)\n",
    "\n",
    "# Each category got a call and a designated axis\n",
    "plot_site_timeseries(df_extent_list[0], ax[0,0])\n",
    "plot_site_timeseries(df_extent_list[1], ax[0,1])\n",
    "plot_site_timeseries(df_extent_list[2], ax[1,0])\n",
    "plot_site_timeseries(df_extent_list[3], ax[1,1])\n",
    "\n",
    "# These are setting labels\n",
    "ax[0,0].set_title('Change in Active Layer Thickness in Continuous Permafrost', fontsize=6)\n",
    "ax[0,0].set_ylabel('Active Layer Thickness in cm',fontsize=6)\n",
    "ax[0,0].set_xlabel('Year',fontsize=6)\n",
    "ax[0,0].tick_params(axis='both', which='major', labelsize=5)\n",
    "ax[0,1].set_title('Change in Active Layer Thickness in Discontinuous Permafrost', fontsize=6)\n",
    "ax[0,1].set_ylabel('Active Layer Thickness in cm',fontsize=6)\n",
    "ax[0,1].set_xlabel('Year',fontsize=6)\n",
    "ax[0,1].tick_params(axis='both', which='major', labelsize=5)\n",
    "ax[1,0].set_title('Change in Active Layer Thickness in Isolated Permafrost', fontsize=6)\n",
    "ax[1,0].set_ylabel('Active Layer Thickness in cm',fontsize=6)\n",
    "ax[1,0].set_xlabel('Year',fontsize=6)\n",
    "ax[1,0].tick_params(axis='both', which='major', labelsize=5)\n",
    "ax[1,1].set_title('Change in Active Layer Thickness in Spororatic Permafrost', fontsize=6)\n",
    "ax[1,1].set_ylabel('Active Layer Thickness in cm',fontsize=6)\n",
    "ax[1,1].set_xlabel('Year',fontsize=6)\n",
    "ax[1,1].tick_params(axis='both', which='major', labelsize=5)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so let's see how the change in ALT varies with permafrost extent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax= plt.subplots(figsize=(4,4), dpi=100)\n",
    "ax.set_ylim([-21,21])\n",
    "fig4=sns.boxplot(data=joined_data, y='slope', x='EXTENT',order=[\"I\", \"S\", \"D\", \"C\"], width=.5).set(title='Slope by Permafrost Extent')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those \"D\" and \"C\" distributions look pretty similar actually! To check it out statistically, Sade performed a [t-test](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the ALT change data as two arrays \n",
    "df_C_slope=joined_data.loc[joined_data['EXTENT']=='C']['slope'].dropna().values\n",
    "df_D_slope=joined_data.loc[joined_data['EXTENT']=='D']['slope'].dropna().values\n",
    "\n",
    "stats.ttest_ind(df_C_slope, df_D_slope)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what's the answer to my term-long query? :)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hackathon prompt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What other relationships can we squeeze out of this dataset? A nasty task is to parse the \"Method\" column to see if the ways that the measurements were made have any bearing on these answers. Will you cross-reference this dataset with another one out in the world? Go for it!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neukom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c4496227e9e35a1cad8f46d1878e766ce3696b74827c1cccb91d7e0ed1733d2b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
