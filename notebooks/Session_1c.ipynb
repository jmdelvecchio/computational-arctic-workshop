{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bringing it all together: cleaning data, do analyses, put those analyses in space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the last portion of this workshop session we will look at a very real example of how my students and I bring together all sorts of exploratory data analysis and tons of different kinds of Python packages - from mathematical packages like `numpy` and `scipy` to data analysis and visualization tools like `pandas` and `seaborn` to geospaital tools in `geopandas`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "import geopandas as gpd\n",
    "import seaborn as sns\n",
    "\n",
    "import scipy as sp\n",
    "from scipy import stats"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sade took this [fairly messy data](https://www2.gwu.edu/~calm/data/north.htm) and turned it into something great!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `CALM_export.csv` is actually an export from the shapefile listed on the website above - it was the least messy format of the data. But I just exported it as a .csv so we had an example of turning columns with location data into `GeoDataframes` :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= pd.read_csv('../arctic-data/CALM_export.csv')\n",
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Major cleaning!!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing Sade did was rename the columns that should be years for the ALT measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_names = np.arange(1990, 2016, 1)\n",
    "\n",
    "old_columns = data.columns[6:32]\n",
    "\n",
    "mapping = {old_columns[i]: year_names[i] for i in range(len(old_columns))}\n",
    "\n",
    "data = data.rename(columns=mapping)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, when imported or exported the \"no measurements\" turned into zeros, which is bad, and some of the data had symbols associated with them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.fillna(np.nan)\n",
    "data = data.replace(r'^\\s*$', np.nan, regex=True)\n",
    "data.replace(0.0, np.nan, inplace= True)\n",
    "data.replace(\">263\", np.nan, inplace= True)\n",
    "data.replace(\">260\", np.nan, inplace= True)\n",
    "data.replace(\">235\", np.nan, inplace= True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we want to make sure our year columns are being read as floats and not objects (strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[:, 6:32].dtypes\n",
    "\n",
    "# iloc is index location\n",
    "# The : in the first half of the bracketed list means \"all rows\"\n",
    "# and then \"column numbers 6 through 32\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops! Let's fix that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[:, 6:32] = data.iloc[:, 6:32].apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How is the active layer changing every year?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to have to get a little clever here, because each site has its own unique dataset issues - some sites are missing most years, some sites have data gaps... "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write an algorithm that grabs an x and y array for years and measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sites in range(len(data)): # for all the rows in our dataset\n",
    "\n",
    "  y_floats = np.array(data.iloc[sites, 6:32].values, dtype=float) # read in the data from columns that are yearly ALT measurements as floats\n",
    "\n",
    "  y = y_floats[~np.isnan(y_floats)] # find the indices for which that year's measurement is NOT a nan\n",
    "\n",
    "  x = year_names[~np.isnan(y_floats)] # find the corresponding years for valid data \n",
    "\n",
    "  if np.sum([np.isnan(y)==False])>10: # if there are at least 10 valid measurements for the time period\n",
    "\n",
    "    data.loc[(sites, \"average\")] = np.mean(y) # grab the mean of those measurements\n",
    "\n",
    "    res = stats.linregress(x,y) # and regress the year array against the measurement array \n",
    "    # res is the result of the linregress function and spits out a list of important numbers\n",
    "\n",
    "    data.loc[(sites, \"slope\")] = res[0] # ...and store all that data in our data frame\n",
    "    data.loc[(sites, \"intercept\")] = res[1]\n",
    "    data.loc[(sites, \"rvalue\")] = res[2]\n",
    "    data.loc[(sites, \"pvalue\")] = res[3]\n",
    "    data.loc[(sites, \"stderr\")] = res[4]\n",
    "  else: # if we don't have enough valid data, go to the next row\n",
    "    continue"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now make it a map!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any dataframe with lats and longs can be convered into a dataframe if we specify the geometry as the appropriate columns and the appropriate crs (lat long will usually be WGS84, EPSG:4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.GeoDataFrame(\n",
    "    data, geometry=gpd.points_from_xy(data.Longitude, data.Latitude), crs='epsg:4326')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what this looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GeoPandas has a simple map of the Earth built in\n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5),dpi=300)\n",
    "im = world.plot(\n",
    "    color='white', edgecolor='black', ax=ax)\n",
    "\n",
    "gdf.plot(\n",
    "        ax=ax,\n",
    "        column='slope',\n",
    "        vmin=-2,\n",
    "        vmax=2,\n",
    "        cmap='seismic',\n",
    "        s=10, #size of point\n",
    "        legend=True,\n",
    "        legend_kwds={\n",
    "            'label': \"Change in Active Layer Thickness (cm/yr) from 1991 to 2015\",\n",
    "            'orientation': \"horizontal\"\n",
    "            }\n",
    "        )\n",
    "\n",
    "ax.set_ylim(40,90)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will leave [axis labeling](https://matplotlib.org/stable/api/axes_api.html) to you :)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's export this as a shapefile for use later down the road"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A goofy little thing where the numbers have to be strings to export lol\n",
    "gdf.columns = gdf.columns.astype(str)\n",
    "\n",
    "gdf.dropna(inplace=True, subset='Site_Name')\n",
    "\n",
    "gdf.to_file(\"CALM_points.shp\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neukom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c4496227e9e35a1cad8f46d1878e766ce3696b74827c1cccb91d7e0ed1733d2b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
